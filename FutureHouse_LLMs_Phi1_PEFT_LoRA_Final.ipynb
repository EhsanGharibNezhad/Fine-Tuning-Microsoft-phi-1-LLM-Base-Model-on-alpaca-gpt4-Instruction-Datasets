{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b4e8d1",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ¤— Fine-Tune Phi-1 LLMs on GSM8K Dataset Using PEFT/LoRA Method\n",
    "\n",
    "###  Investigator: Ehsan (Sam) Gharib-Nezhad\n",
    "\n",
    "[LinkedIn](https://www.linkedin.com/in/ehsan-gharib-nezhad/) &nbsp; \n",
    "[GitHub](https://github.com/EhsanGharibNezhad/) &nbsp; \n",
    "[Hugging Face](https://huggingface.co/ehsangharibnezhad) &nbsp; \n",
    "[Homepage](https://www.nasa.gov/people/ehsan-sam-gharib-nezhad/)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6773e07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a1585",
   "metadata": {},
   "source": [
    "# Experiment Design\n",
    "\n",
    "The experimental design for fine-tuning Language Models (LLMs) involves a set of decisions and configurations made to train a pre-existing language model on a specific task or domain. These key considerations in the experimental design for LLMs fine-tuning will be followed in this notebook:\n",
    "\n",
    "\n",
    "**1. Task Definition:**\n",
    "   - Finetune a LLM (ph1-1.5) for enhanced question answering, with LoRA\n",
    "\n",
    "**2. Dataset Selection:**\n",
    "   - Choose a relevant and representative dataset for your task. The dataset should align with the target domain or application. Ensure that the dataset is labeled appropriately for supervised tasks. We used `vicgalle/alpacagpt4` dataset which includes instructions.\n",
    "\n",
    "**3. Model Selection:**\n",
    "   - Choose a pre-trained language model that serves as the base for fine-tuning. `LLM (ph1-1.5)` were used here.\n",
    "\n",
    "**4. Tokenization and Input Format:**\n",
    "   - Decide on tokenization strategies and input format. Tokenize the input data using the same tokenizer that was used during pre-training. Ensure that the input format matches the requirements of the chosen pre-trained model.\n",
    "\n",
    "**5. Hyperparameter Tuning:**\n",
    "   - Fine-tune hyperparameters such as learning rate, batch size, and the number of training epochs. These parameters can significantly impact model performance. Grid search or random search can be used for hyperparameter tuning.\n",
    "\n",
    "\n",
    "**6. Loss Function:**\n",
    "   - Choose an appropriate loss function for the specific task. Common choices include `Cross-Entropy Loss` for classification tasks.\n",
    "   \n",
    "**8. Evaluation Metric:**\n",
    "   - Define evaluation metrics to assess model performance on the validation or test set. Metrics can vary based on the task and may include accuracy, F1 score, precision, recall, etc.\n",
    "\n",
    "\n",
    "**9. Domain Adaptation (Optional):**\n",
    "   - If fine-tuning for a specific domain, consider techniques for domain adaptation to enhance the model's performance on domain-specific data.\n",
    "\n",
    "**10. Fine-tuning Process:**\n",
    "   - Carry out the fine-tuning process on a computing infrastructure that supports the required computational resources. Utilize distributed training if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6adde",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb770017",
   "metadata": {},
   "source": [
    "### Install and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d5b199-e945-46a5-af19-f97ebb34f1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6b946",
   "metadata": {},
   "source": [
    "### Importing Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9cd8a4-9638-4f44-9dca-695adf57747a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee7b0df-f6d5-44d2-8816-3089501f3419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! You're all set!\n"
     ]
    }
   ],
   "source": [
    "# Make sure you machine utilizes GPU for this LLMs job\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You're all set!\")\n",
    "else:\n",
    "    print(\"CUDA is not available. You need GPU machine to run this fine-tune LLMs project.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37246f2c",
   "metadata": {},
   "source": [
    "### Login to huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f774fda-8166-4249-b865-8252f75cbf78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd74b9dcd5a42bca9bf11c187c93e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73cbe0-43d2-40fb-9f4e-cce541ee0f27",
   "metadata": {},
   "source": [
    "# Download the base LLMs model: `microsoft/phi-1_5` \n",
    "\n",
    "\n",
    "### Model Summary\n",
    "The language model Phi-1.5 is a Transformer with 1.3 billion parameters. It was trained using the same data sources as phi-1, augmented with a new data source that consists of various NLP synthetic texts. When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-1.5 demonstrates a nearly state-of-the-art performance among models with less than 10 billion parameters.\n",
    "\n",
    "We did not fine-tune Phi-1.5 either for instruction following or through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.\n",
    "\n",
    "For a safer model release, we exclude generic web-crawl data sources such as common-crawl from the training. This strategy prevents direct exposure to potentially harmful online content, enhancing the model's safety without RLHF. However, the model is still vulnerable to generating harmful content. We hope the model can help the research community to further study the safety of language models.\n",
    "\n",
    "Phi-1.5 can write poems, draft emails, create stories, summarize texts, write Python code (such as downloading a Hugging Face transformer model), etc.\n",
    "\n",
    "### Intended Uses\n",
    "Given the nature of the training data, Phi-1.5 is best suited for prompts using the QA format, the chat format, and the code format. Note that Phi-1.5, being a base model, often produces irrelevant text following the main answer. In the following example, we've truncated the answer for illustrative purposes only.\n",
    "\n",
    "\n",
    "\n",
    "### Ref:\n",
    "https://huggingface.co/microsoft/phi-1_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56db8289-727c-4692-a4ac-bc99a2fcd4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to download a model, tokenizer, or any other file hosted on the Hugging Face Model Hub.\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e35a390c-a83c-4704-bdc0-9c2e84e25dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62db3fc83a634500bf0d63c5ccfd6f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bccf248843e4429b0b46a53b3a4b132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CODE_OF_CONDUCT.md:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41bc0c9759e40878d215d8cd198d8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SECURITY.md:   0%|          | 0.00/2.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7333776329e240dea02fa25e4a2a5092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794e42de585d4c08a0f6afec9263a12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83ca4ba572642e9a4cfa6446d9175e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NOTICE.md:   0%|          | 0.00/1.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133ddb21b24747e1a2a62c024bb78d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repo_id = 'microsoft/phi-1_5'\n",
    "model_path = \"./phi-1_5\"\n",
    "\n",
    "model_path = snapshot_download(repo_id=repo_id,  # Specify the commit or version you want to download\n",
    "                               repo_type=\"model\",\n",
    "                               local_dir=model_path, # Specify the file or directory path you want to download\n",
    "                               local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e57fc3-a677-4f2a-b91e-e244c116705b",
   "metadata": {},
   "source": [
    "\n",
    "# Deploy the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a74416b-0c15-43ad-9802-93c6773ffe94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# instantiate a tokenizer for a given pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "\n",
    "# Padding is often necessary when working with sequences of varying lengths, such as sentences in NLP.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272566e0-6f33-48ff-912b-f306f694a1d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./phi-1_5\",  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492390d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2048)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the based model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac59fce",
   "metadata": {},
   "source": [
    "### Test the pre-trained Phi-1 LLM: \n",
    "\n",
    "Output should be: \n",
    "![image](phi1_5_example.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19c1149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   primes = []\n",
      "   for num in range(2, n+1):\n",
      "       is_prime = True\n",
      "       for i in range(2, int(math.sqrt(num))+1):\n",
      "           if num % i == 0:\n",
      "               is_prime = False\n",
      "               break\n",
      "       if is_prime:\n",
      "           primes.append(num)\n",
      "   print(primes)\n",
      "   \n",
      "print_prime(20)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "```\n",
      "\n",
      "Exercise 5:\n",
      "Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
      "\n",
      "```python\n",
      "def sum_even(numbers):\n",
      "   \"\"\"\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01931e77-6d61-4fdd-bb80-525daeb1c2fc",
   "metadata": {},
   "source": [
    "## Model Quantization \n",
    "\n",
    "### Why?\n",
    "To reduce the precision of numerical values in a model. Instead of using high-precision data types, such as 32-bit floating-point numbers, quantization represents values using lower-precision data types, such as 8-bit integers. This process significantly reduces memory usage and can speed up model execution while maintaining acceptable accuracy.\n",
    "\n",
    "### Bitsandbytes Library?\n",
    "Hugging Faceâ€™s Transformers library to make the process of model quantization more accessible and empowers users to achieve efficient models with just a few lines of code. The following libraries are installed in the begining of this notebook for this step:\n",
    "\n",
    "- `!pip install -q bitsandbytes`\n",
    "- `!pip install -q accelerate`\n",
    "- `!pip install -q git+https://github.com/huggingface/transformers.git@main` \n",
    "\n",
    "\n",
    "\n",
    "In summary, this code is configuring a language model (presumably using the Phi-1 architecture) for 4-bit quantization with double quantization and a specific quantization type (\"nf4\"). \n",
    "\n",
    "Ref: https://huggingface.co/docs/optimum/concept_guides/quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8fd6b98-9c62-47ac-aeef-041c37ecdf2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quantization techniques reduces memory and computational costs by representing weights and\n",
    "# activations with lower-precision data types like 8-bit integers (int8).\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                   # loaded in 4-bit quantization mode.\n",
    "    bnb_4bit_use_double_quant=True,      # double quantization in 4-bit mode\n",
    "    bnb_4bit_quant_type=\"nf4\",           # is designed for weights initialized using a normal distribution. \n",
    "    bnb_4bit_compute_dtype=torch.float16 # Sets the computation data type to 16-bit floating-point\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-1_5\",\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a926483c-6f03-4174-874c-08f992c7570b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2048)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07139a92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df098dc3",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Pre-Trained Phi-1 LLMs Using PEFT/LoRA\n",
    "\n",
    "### PEFT/LoRA: Low-Rank Adaptation of Large Language Models\n",
    "\n",
    "\n",
    "Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. Fine-tuning large-scale PLMs is often prohibitively costly. In this regard, PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs. \n",
    "\n",
    "Ref: https://github.com/huggingface/peft\n",
    "\n",
    "![image](LoRA_overview.jpg)\n",
    "\n",
    "### Figure Legend: LoRA Overview\n",
    "\n",
    "    1. LoRA can be implemented as an adapter designed to enhance and expand the existing neural network layers. \n",
    "    It introduces an additional layer of trainable parameters (weights) while maintaining the original parameters in a frozen state. These trainable parameters possess a substantially reduced rank (dimension) compared to the dimensions of the original network. This is the mechanism through which LoRa simplifies and expedites the process of adapting the original models for domain-specific tasks. Now, letâ€™s take a closer look at the components within the LORA adapter network.\n",
    "\n",
    "    2. The pre-trained parameters of the original model (W) are frozen. During training, these weights will not be modified.\n",
    "\n",
    "    3. A new set of parameters is concurrently added to the networks WA and WB. These networks utilize low-rank weight vectors, where the dimensions of these vectors are represented as dxr and rxd. Here, â€˜dâ€™ stands for the dimension of the original frozen network parameters vector, while â€˜râ€™ signifies the chosen low-rank or lower dimension. The value of â€˜râ€™ is always smaller, and the smaller the â€˜râ€™, the more expedited and simplified the model training process becomes. Determining the appropriate value for â€˜râ€™ is a pivotal decision in LoRA. Opting for a lower value results in faster and more cost-effective model training, though it may not yield optimal results. Conversely, selecting a higher value for â€˜râ€™ extends the training time and cost, but enhances the modelâ€™s capability to handle more complex tasks.\n",
    "    \n",
    "    4. The results of the original network and the low-rank network are computed with a dot product, which results in a weight matrix of n dimension, which is used to generate the result.\n",
    "    \n",
    "    5. This result is then compared with the expected results (during training) to calculate the loss function and WA and WB weights are adjusted based on the loss function as part of backpropagation like standard neural networks.\n",
    "\n",
    "Ref: \n",
    "- https://abvijaykumar.medium.com/fine-tuning-llm-parameter-efficient-fine-tuning-peft-lora-qlora-part-1-571a472612c4\n",
    "- https://arxiv.org/pdf/2106.09685.pdf\n",
    "\n",
    "### What Libraries? \n",
    "   - `!pip install -q loralib`\n",
    "   - `!pip install -q git+https://github.com/huggingface/peft.git`\n",
    "   \n",
    "### More about LoRA parameters?\n",
    "   - __LoRA Dimension / Rank of Decomposition r__: For each layer to be trained, the d Ã— k weight update matrix âˆ†W is represented by a low-rank decomposition BA, where B is a d Ã— r matrix and A is a r Ã— k matrix. The rank of decomposition r is << min(d,k). The default of r is 8. A is initialized by random Gaussian numbers so the initial weight updates have some variation to start with. B is initialized by by zero so âˆ†W is zero at the beginning of training. \n",
    "   - __Alpha Parameter for LoRA Scaling lora_alpha__:  âˆ†W is scaled by Î± / r where Î± is a constant. When optimizing with Adam, tuning Î± is roughly the same as tuning the learning rate if the initialization was scaled appropriately.\n",
    "   \n",
    "   - __Dropout Probability for LoRA Layers lora_dropout__: Dropout is a technique to reduce overfitting by randomly selecting neurons to ignore with a dropout probability during training. The contribution of those selected neurons to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass. The default of lora_dropout is 0.\n",
    "   - __Bias Type for Lora bias__: Bias can be â€˜noneâ€™, â€˜allâ€™ or â€˜lora_onlyâ€™. If â€˜allâ€™ or â€˜lora_onlyâ€™, the corresponding biases will be updated during training. Even when disabling the adapters, the model will not produce the same output as the base model would have without adaptation. The default is None.\n",
    "   \n",
    "![image](LoRA_parameters2.jpg)\n",
    "\n",
    "Ref: \n",
    "   - https://medium.com/@manyi.yim/more-about-loraconfig-from-peft-581cf54643db\n",
    "   - https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abecc60f-d107-4b76-90d2-0c50e21d12e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,864,320 || all params: 1,426,135,040 || trainable%: 0.5514428703750243\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the target_modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f7d7c",
   "metadata": {},
   "source": [
    "### Review the Model Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a046d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration Parameters:\n",
      "peft_type: LORA\n",
      "auto_mapping: None\n",
      "base_model_name_or_path: microsoft/phi-1_5\n",
      "revision: None\n",
      "task_type: CAUSAL_LM\n",
      "inference_mode: False\n",
      "r: 16\n",
      "target_modules: {'fc2', 'fc1', 'Wqkv', 'out_proj'}\n",
      "lora_alpha: 32\n",
      "lora_dropout: 0.05\n",
      "fan_in_fan_out: False\n",
      "bias: none\n",
      "use_rslora: False\n",
      "modules_to_save: None\n",
      "init_lora_weights: True\n",
      "layers_to_transform: None\n",
      "layers_pattern: None\n",
      "rank_pattern: {}\n",
      "alpha_pattern: {}\n",
      "megatron_config: None\n",
      "megatron_core: megatron.core\n",
      "loftq_config: {}\n"
     ]
    }
   ],
   "source": [
    "# Print configuration parameters\n",
    "print(\"\\nConfiguration Parameters:\")\n",
    "for param, value in peft_config.__dict__.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51763bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d8043e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens: Embedding(51200, 2048)\n",
      "Embedding Dropout: Dropout(p=0.0, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "# Extract embedding components from the model\n",
    "embed_tokens = model.base_model.model.model.embed_tokens\n",
    "embed_dropout = model.base_model.model.model.embed_dropout\n",
    "\n",
    "# Print or use the extracted embedding components\n",
    "print(\"Embedding Tokens:\", embed_tokens)\n",
    "print(\"Embedding Dropout:\", embed_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb783b4-75f4-42ed-ba1d-b7969157132d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_A in fc1: Linear(in_features=2048, out_features=16, bias=False)\n",
      "lora_B in fc1: Linear(in_features=16, out_features=8192, bias=False)\n",
      "lora_A in fc2: Linear(in_features=8192, out_features=16, bias=False)\n",
      "lora_B in fc2: Linear(in_features=16, out_features=2048, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Extract Lora linear layers from the model's first decoder layer\n",
    "lora_A_fc1 = model.base_model.model.model.layers[0].mlp.fc1.lora_A['default']\n",
    "lora_B_fc1 = model.base_model.model.model.layers[0].mlp.fc1.lora_B['default']\n",
    "\n",
    "lora_A_fc2 = model.base_model.model.model.layers[0].mlp.fc2.lora_A['default']\n",
    "lora_B_fc2 = model.base_model.model.model.layers[0].mlp.fc2.lora_B['default']\n",
    "\n",
    "# Print or use the extracted Lora linear layers\n",
    "print(\"lora_A in fc1:\", lora_A_fc1)\n",
    "print(\"lora_B in fc1:\", lora_B_fc1)\n",
    "\n",
    "print(\"lora_A in fc2:\", lora_A_fc2)\n",
    "print(\"lora_B in fc2:\", lora_B_fc2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605500e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40956b",
   "metadata": {},
   "source": [
    "## Load and Tokenize `alpaca-gpt4` dataset\n",
    "\n",
    "This dataset contains English Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\n",
    "\n",
    "The dataset was originaly shared in this repository: https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM. This is just a wraper for compatibility with huggingface's datasets library.\n",
    "\n",
    "`{'instruction': 'Identify the odd one out.',\n",
    "  'input': 'Twitter, Instagram, Telegram',\n",
    " 'output': 'The odd one out is Telegram. Twitter and Instagram are social media platforms mainly for sharing information, images and videos while Telegram is a cloud-based instant messaging and voice-over-IP service.',\n",
    " \n",
    " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nIdentify the odd one out.\\n\\n### Input:\\nTwitter, Instagram, Telegram\\n\\n### Response:\\nThe odd one out is Telegram. Twitter and Instagram are social media platforms mainly for sharing information, images and videos while Telegram is a cloud-based instant messaging and voice-over-IP service.'}`\n",
    "\n",
    "\n",
    "Ref: https://huggingface.co/datasets/vicgalle/alpaca-gpt4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609bcd7",
   "metadata": {},
   "source": [
    "### The parameters passed to the tokenizer function are:\n",
    "- sample[\"text\"]: The text to be tokenized, accessed from the \"text\" key of the sample dictionary.\n",
    "- padding=True: Adds padding tokens to ensure all sequences have the same length.\n",
    "- truncation=True: Truncates sequences longer than the specified max_length.\n",
    "- max_length=512: Specifies the maximum length of the tokenized sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed8d640a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_ds = load_dataset(\"vicgalle/alpaca-gpt4\", split=\"train[:100]\")\n",
    "# train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fe307da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_ds = load_dataset(\"vicgalle/alpaca-gpt4\", split=\"train[100:150]\")\n",
    "# test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75dc3fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f53d7f92a84a25a271208cea8f5a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# Load your dataset\n",
    "train_ds = load_dataset(\"vicgalle/alpaca-gpt4\")\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vicgalle/alpaca-gpt4\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tok(sample):\n",
    "    model_inps = tokenizer(sample[\"text\"], padding=True, truncation=True)\n",
    "    return model_inps\n",
    "\n",
    "# Tokenize the training data\n",
    "tokenized_training_data = train_ds.map(tok, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d33889ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a3d3e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 52002\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_training_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23e8b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_training_data = tokenized_training_data\n",
    "\n",
    "# Set the percentages for train, validation, and test splits\n",
    "train_percentage = 0.8\n",
    "val_percentage = 0.1\n",
    "test_percentage = 0.1\n",
    "\n",
    "# Calculate the number of samples for each split\n",
    "num_samples = len(tokenized_training_data['train'])\n",
    "num_train = int(train_percentage * num_samples)\n",
    "num_val = int(val_percentage * num_samples)\n",
    "num_test = int(test_percentage * num_samples)\n",
    "\n",
    "# Split the tokenized training dataset\n",
    "splits = tokenized_training_data['train'].train_test_split(test_size=(num_val + num_test))\n",
    "tokenized_train_dataset = splits['train']\n",
    "val_test_dataset = splits['test']\n",
    "\n",
    "# Further split the val_test_dataset into validation and test datasets\n",
    "splits_val_test = val_test_dataset.train_test_split(test_size=(num_test / (num_val + num_test)))\n",
    "tokenized_val_dataset = splits_val_test['train']\n",
    "tokenized_test_dataset = splits_val_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5cb08bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 41602\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5716f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access a specific split (e.g., 'train')\n",
    "# split_data = train_ds['train']\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "data_df = tokenized_training_data['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8362e7",
   "metadata": {},
   "source": [
    "#### check the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "538f34ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td></td>\n",
       "      <td>1. Eat a balanced and nutritious diet: Make su...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[21106, 318, 281, 12064, 326, 8477, 257, 4876,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td></td>\n",
       "      <td>The three primary colors are red, blue, and ye...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[21106, 318, 281, 12064, 326, 8477, 257, 4876,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td></td>\n",
       "      <td>An atom is the basic building block of all mat...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[21106, 318, 281, 12064, 326, 8477, 257, 4876,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td></td>\n",
       "      <td>There are several ways to reduce air pollution...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[21106, 318, 281, 12064, 326, 8477, 257, 4876,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Describe a time when you had to make a difficu...</td>\n",
       "      <td></td>\n",
       "      <td>As an AI assistant, I do not have my own perso...</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>[21106, 318, 281, 12064, 326, 8477, 257, 4876,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction input  \\\n",
       "0               Give three tips for staying healthy.         \n",
       "1                 What are the three primary colors?         \n",
       "2                 Describe the structure of an atom.         \n",
       "3                   How can we reduce air pollution?         \n",
       "4  Describe a time when you had to make a difficu...         \n",
       "\n",
       "                                              output  \\\n",
       "0  1. Eat a balanced and nutritious diet: Make su...   \n",
       "1  The three primary colors are red, blue, and ye...   \n",
       "2  An atom is the basic building block of all mat...   \n",
       "3  There are several ways to reduce air pollution...   \n",
       "4  As an AI assistant, I do not have my own perso...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Below is an instruction that describes a task....   \n",
       "1  Below is an instruction that describes a task....   \n",
       "2  Below is an instruction that describes a task....   \n",
       "3  Below is an instruction that describes a task....   \n",
       "4  Below is an instruction that describes a task....   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [21106, 318, 281, 12064, 326, 8477, 257, 4876,...   \n",
       "1  [21106, 318, 281, 12064, 326, 8477, 257, 4876,...   \n",
       "2  [21106, 318, 281, 12064, 326, 8477, 257, 4876,...   \n",
       "3  [21106, 318, 281, 12064, 326, 8477, 257, 4876,...   \n",
       "4  [21106, 318, 281, 12064, 326, 8477, 257, 4876,...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3305cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give three tips for staying healthy.\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n",
      "\n",
      "==================================================\n",
      "\n",
      "[21106   318   281 12064   326  8477   257  4876    13 19430   257  2882\n",
      "   326 20431 32543   262  2581    13   198   198 21017 46486    25   198\n",
      " 23318  1115  9040   329 10589  5448    13   198   198 21017 18261    25\n",
      "   198    16    13 27574   257 12974   290 48102  5496    25  6889  1654\n",
      "   534 13840   389 19889   286   257  4996   286 15921   290 13701    11\n",
      " 10904  7532    11  2187 21824    11   290  5448 27997    13   770  5419\n",
      "   284  2148   534  1767   351   262  6393 20901   284  2163   379   663\n",
      "  1266   290   460  1037  2948 10726 10040    13   198   198    17    13\n",
      "  1985   496   287  3218  3518  3842    25 32900   318  8780   329 10941\n",
      "  1913 11945    11 12749    11   290 21134  1535    13 36223   329   379\n",
      "  1551  6640  2431   286 10768 43294  5517   393  5441  2431   286 31543\n",
      "  5517  1123  1285    13   198   198    18    13  3497  1576  3993    25\n",
      " 18067  1576  3081  3993   318  8780   329  3518   290  5110   880    12\n",
      " 11873    13   632  5419   284 16697 10038    11  2987 10870  2163    11\n",
      "   290  6971  5448  3349   290 10900  2163    13 36223   329   767    12\n",
      "    24  2250   286  3993  1123  1755    13 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256]\n",
      "\n",
      "==================================================\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data_df.iloc[0]\n",
    "\n",
    "# Print the data\n",
    "for entry in data:\n",
    "    print(entry)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator line for better readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa823f8a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bddf336",
   "metadata": {},
   "source": [
    "## Fine-tune the pre-trained Phi-1 model on gsm8k dataset using PEFT/LoRA method\n",
    "\n",
    "### Explanation of key parameters:\n",
    "\n",
    "- **output_dir**: Directory where the model checkpoints and logs will be saved.\n",
    "\n",
    "- **per_device_train_batch_size**: Batch size per GPU (or CPU) device during training. It determines the number of training samples processed in a single forward and backward pass.\n",
    "\n",
    "- **gradient_accumulation_steps**: Number of steps to accumulate gradients before performing a model update. It allows using larger effective batch sizes.\n",
    "\n",
    "- **learning_rate**: Initial learning rate for the optimizer. It controls the step size during optimization.\n",
    "\n",
    "- **lr_scheduler_type**: Learning rate scheduler type. Here, it is set to \"cosine,\" suggesting the use of a cosine annealing learning rate schedule.\n",
    "\n",
    "- **save_strategy**: Strategy for saving checkpoints. In this case, it saves at each epoch.\n",
    "\n",
    "- **logging_steps**: Log metrics every specified number of steps during training.\n",
    "\n",
    "- **max_steps**: Maximum number of training steps. Training will stop when this number is reached.\n",
    "\n",
    "- **num_train_epochs**: Total number of training epochs.\n",
    "\n",
    "- **push_to_hub**: Upload the model checkpoint to the Hugging Face Model Hub after training.\n",
    "\n",
    "- **DataCollatorForLanguageModeling**: object is designed for preparing batches of data suitable for language modeling tasks\n",
    "\n",
    "Ref: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d59057fa-ba57-458f-8a6e-02f5ea14c354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"phi-1_5-finetuned-vicgalle-alpaca-gpt4\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        max_steps=40,\n",
    "        num_train_epochs=5,\n",
    "        report_to=\"none\",\n",
    "        push_to_hub=True\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e347d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f98a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get this followng OOM Error, Try this script to empty the CUDA cache:\n",
    "# ==> OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 22.19 GiB total capacity....\n",
    "\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "979b3992-0038-4a0a-b280-fe1a3cce65d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 32:30, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>1.094744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.982100</td>\n",
       "      <td>1.098478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.950900</td>\n",
       "      <td>1.095425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.036800</td>\n",
       "      <td>1.094171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ehsangharibnezhad/phi-1_5-finetuned-vicgalle-alpaca-gpt4/commit/d620e6ff2638f9bd728b58164344e8ebdd326ba7', commit_message='basic training', commit_description='', oid='d620e6ff2638f9bd728b58164344e8ebdd326ba7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset, #tokenized_data,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=training_arguments,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()\n",
    "# Save the trained model in HuggingFace\n",
    "model.push_to_hub(\"ehsangharibnezhad/phi-1_5-finetuned-vicgalle-alpaca-gpt4\",\n",
    "                  use_auth_token=True,\n",
    "                  commit_message=\"basic training\",\n",
    "                  private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c939e84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiConfig {\n",
       "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
       "  \"architectures\": [\n",
       "    \"PhiForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_phi.PhiConfig\",\n",
       "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_phi.PhiForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": null,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": null,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"phi\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"partial_rotary_factor\": 0.5,\n",
       "  \"qk_layernorm\": false,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.37.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 51200\n",
       "}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac6dbc",
   "metadata": {},
   "source": [
    "### Results Interpretation: \n",
    "While the time and computational resources to fully fine-tune this LLM model with this dataset were not sufficient, the results for 40 epochs show performance metrics of 1.03 and 1.09 for the training and validation sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e18540",
   "metadata": {},
   "source": [
    "## Evlaluate the fine-tuned LLMs performace\n",
    "Cross-entropy loss is a widely used metric to assess the dissimilarity between the predicted probability distribution and the actual distribution of words in the training data. By minimizing cross-entropy loss, the model learns to make more accurate and contextually relevant predictions, particularly in tasks related to text generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c208611-d6a3-40cd-b9de-e809e2c027bf",
   "metadata": {},
   "source": [
    "# Save the trained LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0490f5e-d77a-44a7-a85a-1b83f28550fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e448bc1fa72435190b5ec3931264029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7fcb581c034df7bdc85161c85b5ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/31.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2048)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", \n",
    "                                             trust_remote_code=True, torch_dtype=torch.float32)\n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       \"ehsangharibnezhad/phi-1_5-finetuned-vicgalle-alpaca-gpt4\", \n",
    "                                       from_transformers=True)\n",
    "model = peft_model.merge_and_unload()\n",
    "model\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65e24366-48fb-4f05-a951-05f86f31fdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed6dee630b04831b80d30169eaf9039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c10cc15c94b49d3a78e27e2875bbace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ded7a78898f4fb9af4ec2e01f39e816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/688M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caad09f146441d49b62ebbc0c6d419f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ehsangharibnezhad/phi-1_5-finetuned-vicgalle-alpaca-gpt4/commit/c3c8322d05fc9fc7580f4ab4947ddadf07c89dce', commit_message='Upload PhiForCausalLM', commit_description='', oid='c3c8322d05fc9fc7580f4ab4947ddadf07c89dce', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"ehsangharibnezhad/phi-1_5-finetuned-vicgalle-alpaca-gpt4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07168114",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "602e723c-63bf-4f0e-a7de-3ea553093663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13a0085ce724f22a9f2c3efe1b1be57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design a pipline to monitor the performace in a tech company \n",
      "Answer: The pipline should be designed to monitor the performance of the company in real-time, with sensors placed at strategic locations to track metrics such as productivity, efficiency, and customer satisfaction. The data collected can be analyzed using machine learning algorithms to identify trends and patterns, and the company can use this information to make informed decisions about how to improve their operations.\n",
      "\n",
      "Exercise: What are some potential risks of using a pipline to monitor the performance of a tech company? \n",
      "Answer: Some potential risks of using a pipline to monitor the performance of a tech company include data breaches, privacy violations, and the potential for the data to be misused or misinterpreted. Additionally, the use of a pipline may create a culture of constant surveillance and micromanagement, which can be detrimental to employee morale and productivity.\n",
      "\n",
      "Exercise: How can a pipline be used to improve the performance of a tech company? \n",
      "Answer: A pipline can be used to improve the performance of a tech company by providing real-time data on metrics such as productivity, efficiency, and customer satisfaction. This data can be used to identify areas for improvement and make informed decisions about how to optimize operations. Additionally, the use of a pipline can help to foster a culture of continuous improvement and innovation, as employees are encouraged to share their ideas and insights in real-time.\n",
      "<|endoftext|>\n",
      "\n",
      "\n",
      "Title: The Fascinating World of Mathematics: Exploring the Wonders of Addition\n",
      "\n",
      "Introduction:\n",
      "Welcome, dear Alien friend, to the intriguing realm of mathematics! Today, we embark on a journey to unravel the mysteries of addition, a fundamental operation that forms the basis of countless mathematical concepts. Just as you may have your own unique ways of understanding the world, we humans have developed a language of numbers and symbols to communicate and solve problems. So, let's dive into the captivating world of addition, where numbers come together to create something greater than the sum of its parts.\n",
      "\n",
      "Chapter 1: The Basics of Addition\n",
      "In our world, addition is the process of combining two or more numbers to find their total or sum. It is represented by the symbol \"+\". For example, if we have two apples and add three more, we end up with a total of five apples. This concept of combining quantities is crucial in our daily lives, from counting money to measuring ingredients for a recipe.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ehsangharibnezhad/phi-1_5-finetuned-vicgalle-alpaca-gpt4\", trust_remote_code=True, torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "inputs = tokenizer('''Design a pipline to monitor the performace in a tech company ''', \n",
    "                       return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=512)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be15f6b2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold clips to 48 of her friends in April, and then she sold half as \n",
      "many clips in May. How many clips did Natalia sell altogether in April and May? \n",
      "Answer: In April, Natalia sold 48 clips because 48/2=<<48/2=24>>24 clips\n",
      "In May, Natalia sold 24 clips because 24/2=<<24/2=12>>12 clips\n",
      "Natalia sold 48+24+12=<<48+24+12=72>>72 clips\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April and May\n",
      "#### 72 clips in total\n",
      "#### 72 clips in April\n"
     ]
    }
   ],
   "source": [
    "# inputs = tokenizer('''Suggest a tutorial to expert machine learning ''', \n",
    "#                    return_tensors=\"pt\", \n",
    "#                    return_attention_mask=False)\n",
    "\n",
    "# outputs = model.generate(**inputs, max_length=512)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03c7a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbda7c",
   "metadata": {},
   "source": [
    "# Key questions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068f1b6",
   "metadata": {},
   "source": [
    "### 1- Does Phi-1.5 have any unique model components making Lora difficult?\n",
    "\n",
    "As discussed in the Hugging Face model profile for microsoft/Phi1_5 and phi1, this model is specialized for basic Python coding. Its training encompassed a diverse range of data sources, including subsets of Python code from The Stack v1.2, Q&A content from StackOverflow, competition code from code contests, and synthetic Python textbooks and exercises generated by gpt-3.5-turbo-0301. However, it is worth noting that the model lacks reliability in responding to instructions since it has not undergone instruction fine-tuning. Consequently, it may encounter difficulties or fail to adhere to intricate or nuanced instructions provided by users.\n",
    "\n",
    "On the other hand, the implemented datasets in this project, vicgalle/alpaca-gpt4, are instruction-based text sets. The addition of LoRA involves incorporating a few layers to the base model to customize it for instruction datasets used in this project. It's important to acknowledge that this customization might not guarantee full accuracy, as LoRA's effectiveness in enhancing model performance on specific instruction datasets may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20af0d",
   "metadata": {},
   "source": [
    "### 2- How can we show progress with/without GPU in that amount of time?\n",
    "\n",
    "I utilized the quantization method to reduce the precision of numerical values in a model. Instead of using high-precision data types, such as 32-bit floating-point numbers, quantization represents values using lower-precision data types, such as 8-bit integers (as discussed earlier in this notebook). Additionally, the training steps in the LoRA fine-tuning process are limited to a few tens, but training and validation sets are employed to monitor the performance of the PEFT/LoRA fine-tuning method. However, all of these processes are performed using a GPU at SageMaker (ml.g5.4xlarge). In a scenario without GPU access, this would be very cumbersome. In your Python script or notebook, set the device to \"cpu\" using PyTorch, as follows:\n",
    "\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set device to GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    # Set device to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdf2f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea28d44",
   "metadata": {},
   "source": [
    "### What are the limitations of the current implementation?\n",
    "\n",
    "- **Limitation with the base model**: phi1_5, is not a good choice to begin with and fine-tune it with instruction dataset\n",
    "- **LoRA hyperparametrization**: The performance of LoRA is sensitive to hyperparameters, such as the rank of the low-rank decomposition matrices. Choosing appropriate hyperparameter values can be challenging and may require experimentation. \n",
    "- **Dataset**: Instruction promepts in the dataset may not be sufficient to fine-tune the pre-trained model. In addition, it might not cover all topics. This *alpaca-gpt4* contains 52K instruction-following data generated by GPT-4 using the same prompts as in Alpaca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bdc242",
   "metadata": {},
   "source": [
    "#### Thank you for checking my project and going through it!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3556bb",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
